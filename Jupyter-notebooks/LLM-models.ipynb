{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\thenu\\appdata\\local\\anaconda3\\envs\\internship\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.25.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\thenu\\appdata\\local\\anaconda3\\envs\\internship\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\thenu\\appdata\\local\\anaconda3\\envs\\internship\\lib\\site-packages (from transformers) (24.1)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.9.11-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\thenu\\appdata\\local\\anaconda3\\envs\\internship\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\thenu\\appdata\\local\\anaconda3\\envs\\internship\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\thenu\\appdata\\local\\anaconda3\\envs\\internship\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Downloading charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "   ---------------------------------------- 0.0/9.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.6/9.5 MB 7.7 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.4/9.5 MB 5.6 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.7/9.5 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.9/9.5 MB 5.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 4.2/9.5 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.5/9.5 MB 3.7 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.0/9.5 MB 3.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.0/9.5 MB 3.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.5/9.5 MB 2.8 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.8/9.5 MB 2.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 7.1/9.5 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.7/9.5 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.5/9.5 MB 3.4 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.25.0-py3-none-any.whl (436 kB)\n",
      "Downloading PyYAML-6.0.2-cp312-cp312-win_amd64.whl (156 kB)\n",
      "Downloading regex-2024.9.11-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Downloading safetensors-0.4.5-cp312-none-win_amd64.whl (286 kB)\n",
      "Downloading tokenizers-0.19.1-cp312-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 1.3/2.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 6.0 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Downloading charset_normalizer-3.3.2-cp312-cp312-win_amd64.whl (100 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: urllib3, tqdm, safetensors, regex, pyyaml, idna, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed certifi-2024.8.30 charset-normalizer-3.3.2 huggingface-hub-0.25.0 idna-3.10 pyyaml-6.0.2 regex-2024.9.11 requests-2.32.3 safetensors-0.4.5 tokenizers-0.19.1 tqdm-4.66.5 transformers-4.44.2 urllib3-2.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs.huggingface.co/bert-base-uncased/68d45e234eb4a928074dfd868cead0219ab85354cc53d20e772753c6bb9169d3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model.safetensors%3B+filename%3D%22model.safetensors%22%3B&Expires=1727097445&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNzA5NzQ0NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9iZXJ0LWJhc2UtdW5jYXNlZC82OGQ0NWUyMzRlYjRhOTI4MDc0ZGZkODY4Y2VhZDAyMTlhYjg1MzU0Y2M1M2QyMGU3NzI3NTNjNmJiOTE2OWQzP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiJ9XX0_&Signature=Athr0SHpewMp88IF%7E65tASPTwIHYlDr4fFYMkPWfyI-PgKT4mdgpdu%7EbnLUTsiNGAELZnoE6eKD0yMf9tt%7EdWkby2u4DfVheh6FnaF7bx0Yf%7E15sdpBfX%7El6gmfDoBBv6sMfRakUBtzqEYJklZaoFAZt0-J-hkHrp-nSqLiiI-EbgbYZqgDi1pHEScexmjLBzILYf6hcL7O0Zrj-RcRrSjkT%7Ex5JL5c1lUZ%7EIH%7E-PMqgx%7ETbc0gkOrmj6ISZgWnLLNMj2yJVtCo417NDs1HEpN1Lt5AyeF5W5HuMwC64hCjrZ5aR3AH65fR6WiFTrtLBUlb5VL7cuf7-1oSLbV7b8w__&Key-Pair-Id=K3ESJI6DHPFC7: HTTPSConnectionPool(host='cdn-lfs.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, how are you? hello? hello? hello? hello? hello? hello? hello? hello? hello? hello? hello? hello? hello? hello? hello? hello? hello? hello? hello? hello! hello! hello! hello! hello! hello!!!!!!!!! hello!!! hello! hello!!!!!!!!!!!!!!! hello!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel,GPT2Tokenizer, BertTokenizer,BertForMaskedLM\n",
    "\n",
    "def gpt2_generate_response(input_text):\n",
    "    tokenizer=GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model=GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    input_ids=tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    output=model.generate(input_ids, max_length=100, num_retun_sequences=1)\n",
    "    genrated_text=tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return genrated_text\n",
    "\n",
    "def bert_generate_text(input_text):\n",
    "    tokenizer=BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model=BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "    input_ids=tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    output=model.generate(input_ids, max_length=100, num_return_sequences=1,pad_token_id=tokenizer.eos_token_id)\n",
    "    generated_text=tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "user_input=\"Hello, How are you?\"\n",
    "response=bert_generate_text(user_input) #Bert used for somthing predict word\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Thenu\\AppData\\Local\\anaconda3\\envs\\internship\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Thenu\\AppData\\Local\\anaconda3\\envs\\internship\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted word: ##hood\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer,BertForMaskedLM\n",
    "import torch\n",
    "\n",
    "def predict_masked_word(sentence):\n",
    "    # Load pre-trained BERT model and Tokenizer\n",
    "    tokenizer=BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model=BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    #Tokenize input sentence and find the index of the masked token\n",
    "    input_ids=tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "    mask_token_index=torch.where(input_ids == tokenizer.mask_token_id)[1] \n",
    "\n",
    "    #predict the mask token\n",
    "    with torch.no_grad():\n",
    "        output= model(input_ids)\n",
    "\n",
    "    #Get the logits for the masked token and find the top prediction\n",
    "    mask_token_logits= output.logits[0, mask_token_index, :]\n",
    "    top_token_id=torch.argmax(mask_token_logits, dim=-1)\n",
    "\n",
    "    #Decode the predicted token ID back to a word\n",
    "    predicted_token=tokenizer.decode(top_token_id)\n",
    "    return predicted_token\n",
    "\n",
    "sentence=\"once upon a time, there was person[MASK]?\"\n",
    "predicted_word=predict_masked_word(sentence)\n",
    "print(f\"Predicted word: {predicted_word}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a time when the world was still a little bit more open to the idea of a world where people could live in peace and harmony.\n",
      "\n",
      "\"But now, we have a world where people are not afraid to be themselves. We have a world where people are not afraid to be themselves. We have a world\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "def gpt2_generate_response(input_text):\n",
    "    tokenizer=GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model=GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "    input_ids=tokenizer.encode(input_text, return_tensors='pt')\n",
    "    output=model.generate(input_ids, max_length=65, num_return_sequences=1)\n",
    "\n",
    "    generated_text=tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "input_text=\"There was a time\"\n",
    "generated_text=gpt2_generate_response(input_text)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Thenu\\AppData\\Local\\anaconda3\\envs\\internship\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochh1: Loss- 0.10580132901668549\n",
      "Epochh2: Loss- 0.0662732794880867\n",
      "Epochh3: Loss- 0.056707579642534256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('fine_tuned_gpt2\\\\tokenizer_config.json',\n",
       " 'fine_tuned_gpt2\\\\special_tokens_map.json',\n",
       " 'fine_tuned_gpt2\\\\vocab.json',\n",
       " 'fine_tuned_gpt2\\\\merges.txt',\n",
       " 'fine_tuned_gpt2\\\\added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "#Load pretrained model and tokenizer\n",
    "tokenizer=GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model=GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=1024):\n",
    "        self.tokenizer=tokenizer\n",
    "        self.input_ids=[]\n",
    "\n",
    "        for text in texts:\n",
    "            tokenized_text=tokenizer.encode(text,truncation=True, max_length=max_length)\n",
    "            self.input_ids.append(tokenized_text)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.input_ids[idx])\n",
    "    \n",
    "#Example input texts\n",
    "texts=[\n",
    "    \"Your very long input text here.\" * 100,\n",
    "    \"Another example of a long input text\" *100\n",
    "]\n",
    "\n",
    "#create dataset and dataloader\n",
    "dataset=TextDataset(texts, tokenizer)\n",
    "dataloader=DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# fine tuning parameters\n",
    "optimizer=torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "epochs=3\n",
    "\n",
    "#fine-tuning loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids=batch.squeeze(0) #Remove batch dimension\n",
    "        outputs=model(input_ids, labels=input_ids)\n",
    "        loss=outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epochh{epoch+1}: Loss- {loss.item()}\")\n",
    "\n",
    "#save fine tune model\n",
    "model.save_pretrained(\"fine_tuned_gpt2\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_gpt2\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a time when the world was still a little bit more open than it is now.\n",
      "\n",
      "\"I think it's a good thing that we're not going to have to worry about that anymore,\" he said. \"We're going to\n"
     ]
    }
   ],
   "source": [
    "finetuned_model=GPT2LMHeadModel.from_pretrained('./fine_tuned_gpt2')\n",
    "finetuned_tokenizer=GPT2Tokenizer.from_pretrained('./fine_tuned_gpt2')\n",
    "\n",
    "input_text=\"There was a time\"\n",
    "input_ids=finetuned_tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "output=finetuned_model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
    "\n",
    "print(finetuned_tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"cerebras/Cerebras-GPT-1.3B\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"cerebras/Cerebras-GPT-1.3B\")\n",
    "\n",
    "# text = \"Generative AI is \"\n",
    "# from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "# generated_text = pipe(text, max_length=50, do_sample=False, no_repeat_ngram_size=2)[0]\n",
    "# print(generated_text['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# # Load pre-trained model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"cerebras/Cerebras-GPT-1.3B\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"cerebras/Cerebras-GPT-1.3B\")\n",
    "\n",
    "# device = torch.device(\"cuda\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1866410 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "block_size = 8\n",
    "batch_size = 8\n",
    "\n",
    "with open('shakepeare_s_plays.txt', 'r') as f:\n",
    "  input_text = f.read()\n",
    "\n",
    "texts = input_text.split('\\n')\n",
    "dataset = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches():\n",
    "    indexes=torch.randint(len(dataset[0])- block_size, (batch_size,))\n",
    "    x=torch.stack([dataset[0][i:i+block_size] for i in indexes])\n",
    "    y = torch.stack([dataset[0][i+1:i+block_size+1] for i in indexes]) # [1000112:1000112+block_size, 10:10+block_size, 1823789127, 13912]\n",
    "    # x = x.to(device)\n",
    "    # y = y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss- 7.557468891143799\n",
      "Epoch 2: Loss- 7.041920185089111\n",
      "Epoch 3: Loss- 6.730448246002197\n"
     ]
    }
   ],
   "source": [
    "epochs=3\n",
    "#fine- tuning loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    # print(dataloader)\n",
    "    for batch in range(batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        x,y=batches()\n",
    "        output=model(x, labels=y)\n",
    "        loss=output.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}: Loss- {loss.item()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fine_tuned_gpt2_\\\\tokenizer_config.json',\n",
       " 'fine_tuned_gpt2_\\\\special_tokens_map.json',\n",
       " 'fine_tuned_gpt2_\\\\vocab.json',\n",
       " 'fine_tuned_gpt2_\\\\merges.txt',\n",
       " 'fine_tuned_gpt2_\\\\added_tokens.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save fine-tuned model\n",
    "model.save_pretrained(\"fine_tuned_gpt2_\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_gpt2_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_model = GPT2LMHeadModel.from_pretrained(\"./fine_tuned_gpt2_\")\n",
    "finetuned_tokenizer = GPT2Tokenizer.from_pretrained(\"./fine_tuned_gpt2_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a time when I was a little bit\n"
     ]
    }
   ],
   "source": [
    "input_text = \"There was a time\"\n",
    "input_ids = finetuned_tokenizer.encode(input_text, return_tensors='pt')\n",
    "output = finetuned_model.generate(input_ids, max_length=10, num_return_sequences=1)\n",
    "\n",
    "print(finetuned_tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "internship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
